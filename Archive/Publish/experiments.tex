\documentclass[11pt]{article}

% ---------- Formatting: Springer-like A4, two-column ----------
\usepackage[a4paper,left=18mm,right=18mm,top=22mm,bottom=24mm]{geometry}
\setlength{\columnsep}{5mm} % ~0.20in between columns
\usepackage{microtype}

% ---------- Packages ----------
\usepackage{amsmath,amssymb,mathtools,bm}
\usepackage{siunitx}

% ---------- APA citations ----------
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber,style=apa,sorting=nyt]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{refs.bib} % <-- ensure your BibTeX entries are here

% ---------- Macros ----------
% Sets & basic math
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}

% Bold vectors/matrices
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}

% Common symbols
\newcommand{\x}{\vect{x}}
\newcommand{\z}{\vect{z}}
\newcommand{\w}{\vect{w}}
\newcommand{\muvec}{\bm{\mu}}
\newcommand{\sigvec}{\bm{\sigma}}
\newcommand{\SigmaMat}{\bm{\Sigma}}
\newcommand{\Wmat}{\mat{W}}
\newcommand{\Lambdamat}{\bm{\Lambda}}

% Operators, norms, inner products
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\inner}[2]{\left\langle #1,\, #2 \right\rangle}

% Indicator and sign
\newcommand{\Ind}[1]{\mathbb{I}\!\left\{ #1 \right\}}
\DeclareMathOperator{\sign}{sign}

% Vectorization and PCA whitening
\DeclareMathOperator{\vecop}{vec}
\newcommand{\vecx}{\vecop}
\newcommand{\whiten}[1]{\Lambdamat^{-1/2}\Wmat^{\top} #1}

% Kernel & OC-SVM decision function
\newcommand{\rbf}[2]{\exp\!\big(-\gamma \norm{#1-#2}^{2}\big)}
\newcommand{\fsvm}{f}                % OC-SVM decision function
\newcommand{\score}[1]{-\,\fsvm(#1)} % anomaly-positive score s(z) = -f(z)

% Splits & bands
\newcommand{\train}{\textsc{Train}}
\newcommand{\val}{\textsc{Val}}
\newcommand{\final}{\textsc{Final}}
\newcommand{\bandmean}{\mu_b}
\newcommand{\bandsd}{\sigma_b}

% Metrics & thresholds
\newcommand{\AUC}{\mathrm{AUC}}
\newcommand{\FPzero}{\mathrm{FP@0}}
\newcommand{\topt}{\tau^\ast}

% Dimensions
\newcommand{\Hres}{64}
\newcommand{\Wres}{64}
\newcommand{\Bbands}{6}
\newcommand{\dfeat}{d} % post-processing feature dimension

% ---------- Formatting ----------
\usepackage{authblk} % for journal-style author/affiliation formatting

% ---------- Title ----------
\title{\textbf{From Tensors to Novelties: Low-Dimensional Representations for Anomaly Detection in Multispectral Imagery}}

\author{Anthony Chan}
\affil{Georgia Institute of Technology, OMSA ISyE6748 Summer 2024 \\
\texttt{anthonycchan@gmail.com}}

\date{} % leave empty to omit date, or use \today

\begin{document}
\maketitle

\section{Experiments}
\label{sec:experiments}

\subsection{Overview}
In Section~3.1, I define the dataset partitioning strategy; Section~3.2 defines the CP decomposition on the datasets; Section~3.3 defines the Tucker decomposition; and Section~3.4 defines standardization of the decomposed datasets. The following sections describe the model training and tests performed using the common decomposed training, validation, and test datasets. I consolidate the discussions for models using CP and Tucker decompositions since they share the same anomalyâ€“detection models and differ only in the input decomposition technique. Two dataset configurations are evaluated: (i) an \textit{in-order} split using the first 1,500 training tiles, and (ii) a \textit{randomized} selection drawn from the full dataset to assess robustness to sampling diversity.

\subsection{CP/Tucker Rank Search}
For CP decomposition, I sweep the rank \(R \in \{5,10,15,\dots,380\}\).
For Tucker decomposition, I sweep \((R_1,R_2,R_3)\) where
\(R_1,R_2 \in \{5,16,32,64\}\) and \(R_3 \in \{5,16\}\).
Please see Sections~3.2 and~3.3 for the rationale behind these rank choices and for details of how each decomposition is applied to the training, validation, and test sets.

\subsection{CP/Tucker Decomposition + OC\texorpdfstring{\textnormal{-}}{}SVM}
Given the decomposed training, validation, and test datasets, I fit an OC\mbox{--}SVM on the decomposed training data for each decomposition rank and perform a grid search over:
\[
\gamma \in \Bigl\{ \tfrac{t}{d} : t \in \{0.1,\,0.3,\,1,\,3,\,10\} \Bigr\} \cup \{\texttt{scale}, \texttt{auto}\},
\qquad
\nu \in \{0.01,\,0.02,\,0.05,\,0.10,\,0.20\},
\]
where \(d\) is the feature dimension.

\paragraph{Model selection on typical-only validation.}
Let \(s_{\text{val}}\) be the anomaly scores on the validation set (higher means more anomalous). Because validation contains only typical samples, I select the model by lexicographic minimization:
\[
\min \bigl(\mathrm{FP}@0,\; \mathrm{P95}(s_{\text{val}}),\; \mathrm{mean}(s_{\text{val}})\bigr),
\]
where \(\mathrm{FP}@0\) is the false-positive rate at threshold \(0\), \(\mathrm{P95}\) is the 95th percentile, and \(\mathrm{mean}\) is the sample mean. Intuitively, I first minimize false alarms; ties are broken by preferring tighter and lower score distributions on normals. The rank and OC\mbox{--}SVM hyperparameters achieving this criterion are then evaluated on the test set, and ROC--AUC is reported. Separate rank searches are performed for the in-order and randomized dataset configurations to compare stability and generalization.

\subsection{CP/Tucker Decomposition + Autoencoder}
Given the decomposed training, validation, and test datasets, I fit the autoencoder (defined in Section~3.6) on the training data and validate on the validation set. The optimal rank is chosen as the one yielding the smallest reconstruction error on validation. The chosen rank is then evaluated on the test dataset, and ROC--AUC is reported. This procedure is repeated for both the in-order and randomized splits.

\subsection{CP/Tucker Decomposition + Isolation Forest}
Given the decomposed training, validation, and test datasets, I fit an Isolation Forest on the training data for each decomposition rank and perform a grid search to minimize the validation anomaly score. The selected model is then evaluated on the test dataset. The grid is:
\[
T \in \{50,100,200\},\quad
p_{\text{sub}} \in \{0.5,0.75,1.0\},\quad
m_f \in \{0.5,0.75,1.0\},\quad
\text{contamination} \in \{0.05,0.10,0.20\}.
\]
As with the other models, evaluations are performed separately on both dataset configurations.

\subsection{Raw-Pixel Anomaly Detection}
To establish baselines, I also run OC\mbox{--}SVM, the autoencoder, and Isolation Forest directly on raw-pixel datasets (without decomposition). These baselines allow comparison to determine whether decomposition-based features provide an advantage for anomaly detection under both in-order and randomized splits.

\subsection{Reproducibility and Compute}
I make the code available to aid reproducibility. Experiments were run on the following hardware:
\begin{itemize}
  \item Microsoft Azure D16 Standard: 12 CPU, 64\,GB RAM, no GPU.
  \item Dell workstation: 12 CPU, 64\,GB RAM, NVIDIA RTX A3000 GPU.
\end{itemize}


\end{document}
