\documentclass[11pt]{article}

% ---------- Formatting: Springer-like A4, two-column ----------
\usepackage[a4paper,left=18mm,right=18mm,top=22mm,bottom=24mm]{geometry}
\setlength{\columnsep}{5mm} % ~0.20in between columns
\usepackage{microtype}

% ---------- Packages ----------
\usepackage{amsmath,amssymb,mathtools,bm}
\usepackage{siunitx}

% ---------- APA citations ----------
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber,style=apa,sorting=nyt]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{refs.bib} % <-- ensure your BibTeX entries are here

% ---------- Macros ----------
% Sets & basic math
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}

% Bold vectors/matrices
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}

% Common symbols
\newcommand{\x}{\vect{x}}
\newcommand{\z}{\vect{z}}
\newcommand{\w}{\vect{w}}
\newcommand{\muvec}{\bm{\mu}}
\newcommand{\sigvec}{\bm{\sigma}}
\newcommand{\SigmaMat}{\bm{\Sigma}}
\newcommand{\Wmat}{\mat{W}}
\newcommand{\Lambdamat}{\bm{\Lambda}}

% Operators, norms, inner products
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\inner}[2]{\left\langle #1,\, #2 \right\rangle}

% Indicator and sign
\newcommand{\Ind}[1]{\mathbb{I}\!\left\{ #1 \right\}}
\DeclareMathOperator{\sign}{sign}

% Vectorization and PCA whitening
\DeclareMathOperator{\vecop}{vec}
\newcommand{\vecx}{\vecop}
\newcommand{\whiten}[1]{\Lambdamat^{-1/2}\Wmat^{\top} #1}

% Kernel & OC-SVM decision function
\newcommand{\rbf}[2]{\exp\!\big(-\gamma \norm{#1-#2}^{2}\big)}
\newcommand{\fsvm}{f}                % OC-SVM decision function
\newcommand{\score}[1]{-\,\fsvm(#1)} % anomaly-positive score s(z) = -f(z)

% Splits & bands
\newcommand{\train}{\textsc{Train}}
\newcommand{\val}{\textsc{Val}}
\newcommand{\final}{\textsc{Test}}
\newcommand{\bandmean}{\mu_b}
\newcommand{\bandsd}{\sigma_b}

% Metrics & thresholds
\newcommand{\AUC}{\mathrm{AUC}}
\newcommand{\FPzero}{\mathrm{FP@0}}
\newcommand{\topt}{\tau^\ast}

% Dimensions
\newcommand{\Hres}{64}
\newcommand{\Wres}{64}
\newcommand{\Bbands}{6}
\newcommand{\dfeat}{d} % post-processing feature dimension

\begin{document}

\section{Methodology}\label{sec:methodology}

\subsection{Global CP (PARAFAC) Decomposition}

I apply a global CANDECOMP/PARAFAC (CP) decomposition to the \emph{training} stack and then project the \emph{validation} and \emph{test} tiles into that basis. Let the sample-stacked tensor be \(\mathcal{X}\in\R^{N\times I\times J\times K}\) (samples \(\times\) height \(\times\) width \(\times\) bands). The CP model approximates
\[
\mathcal{X} \;\approx\; \sum_{r=1}^{R}\lambda_r \, h_r \!\circ a_r \!\circ b_r \!\circ c_r,
\]
with factor matrices \(H\in\R^{N\times R}\), \(A\in\R^{I\times R}\), \(B\in\R^{J\times R}\),
\(C\in\R^{K\times R}\), and nonnegative component weights \(\boldsymbol{\lambda}\in\R^{R}\).
Here, each sample (tile) corresponds to a row of \(H\).
For the training data, \(N=1500\), \(I=J=64\), \(K=6\); the validation and test splits each use \(N=200\) with the same spatial/spectral sizes.

\noindent The rank \(R\) is selected during model tuning (see downstream sections), with the practical constraint
\[
R \;\le\; \min(IJ,\;IK,\;JK) \;=\; \min(4096,\;384,\;384) \;=\; 384
\]
for the \((I,J,K)=(64,64,6)\) setting \parencite{KoldaBader2009,Bro1997}.

\noindent To obtain scale-consistent per-sample coefficients, I absorb the weights into the sample-mode factor:
\[
\tilde H \;=\; H\,\mathrm{diag}(\boldsymbol{\lambda}),
\]
so that each row of \(\tilde H\) reflects the absolute contribution of each component.

\paragraph{Projection of validation/test tiles.}
Given a new tile \(X\in\R^{I\times J\times K}\) and the training-fitted basis \((A,B,C)\),
define the component overlaps
\[
g_r \;=\; \big\langle X,\; a_r \circ b_r \circ c_r \big\rangle,\qquad r=1,\dots,R,
\]
and the Hadamard Gram matrix
\[
G \;=\; (A^\top A)\,\ast\,(B^\top B)\,\ast\,(C^\top C).
\]
The least-squares CP coefficients for that tile are
\[
z \;=\; G^{-1} g \;\in\; \R^{R}.
\]
Stacking all projected rows gives the feature matrix
\[
Z \;=\; \begin{bmatrix} z_1^{\top}\\ \vdots \\ z_N^{\top}\end{bmatrix}
\;\in\; \R^{N\times R},
\]
the representation used by downstream models \parencite{Kiers2000,SorensenBro2009}.

\subsection{Tucker Decomposition}

Tucker decomposition is applied to each tile \(X_n \in \R^{64\times 64\times 6}\) with ranks \((r_1,r_2,r_3)\).
The model represents each tile via a core tensor and three orthonormal bases \parencite{Tucker1966,KoldaBader2009}:
\[
X_n \;\approx\; \hat X_n \;=\; \mathcal{G}_n \times_{1} U_1 \times_{2} U_2 \times_{3} U_3,
\qquad U_i^\top U_i = I_{r_i},
\]
with dimensions
\[
\mathcal{G}_n \in \R^{r_1\times r_2\times r_3},\quad
U_1 \in \R^{64\times r_1},\quad
U_2 \in \R^{64\times r_2},\quad
U_3 \in \R^{6\times r_3}.
\]
Here, \(\times_n\) is the mode-\(n\) tensor--matrix product.

\noindent Using the sample-stacked tensor \(\mathcal{X}\), I compute mode-\(n\) unfoldings \(\mathcal{X}_{(n)}\) and obtain \(U_n\) as the leading left singular vectors (HOSVD) \parencite{DeLathauwer2000a}:
\[
\begin{aligned}
\mathcal{X}_{(1)} &= U_1\,S_1\,V_1^\top \;\;\Rightarrow\;\; U_1 = U_1(:,1{:}r_1),\\[4pt]
\mathcal{X}_{(2)} &= U_2\,S_2\,V_2^\top \;\;\Rightarrow\;\; U_2 = U_2(:,1{:}r_2),\\[4pt]
\mathcal{X}_{(3)} &= U_3\,S_3\,V_3^\top \;\;\Rightarrow\;\; U_3 = U_3(:,1{:}r_3).
\end{aligned}
\]

\noindent With \(U_1,U_2,U_3\) fixed, each tile core is
\[
\boxed{\;\mathcal{G}_n \;=\; X_n \times_{1} U_1^{\top} \times_{2} U_2^{\top} \times_{3} U_3^{\top}\; }.
\]
Vectorizing cores yields features
\[
z_n \;=\; \operatorname{vec}(\mathcal{G}_n) \;\in\; \R^{\,r_1 r_2 r_3},\qquad
Z \;=\; \begin{bmatrix} z_1^\top \\ \vdots \\ z_N^\top \end{bmatrix}
\;\in\; \R^{\,N \times (r_1 r_2 r_3)}.
\]

\subsection{Standardization}

Given CP/Tucker features for training, validation, and test, I standardize features using training statistics and reuse the same transform for validation/test to avoid leakage:
\[
\tilde{z} \;=\; S(z) \;=\; \frac{z - \mu_{\text{train}}}{\sigma_{\text{train}}} \;\in\; \R^{d},
\]
where \(z\in\R^{d}\), and \(\mu_{\text{train}},\sigma_{\text{train}}\in\R^{d}\) are computed on training features only.

\subsection{One-Class SVM (RBF Kernel)}

Let \(\tilde{Z}_{\text{train}}\in\R^{|T|\times d}\) be the standardized training feature matrix. I train an OC-SVM with RBF kernel \(K(z,z')=\exp\{-\gamma\lVert z-z'\rVert_2^2\}\) \parencite{Scholkopf2001,TaxDuin2004}, solving
\[
\max_{\alpha}\; -\tfrac{1}{2} \sum_{i,j=1}^{n} \alpha_i \alpha_j K(z_i, z_j)
\quad \text{s.t.} \quad
0 \leq \alpha_i \leq \tfrac{1}{\nu n}, \qquad
\sum_{i=1}^{n} \alpha_i = 1.
\]
Since validation data are typical-only, I minimize the false-positive rate at the default decision boundary \(f(z)=0\) \parencite{Scholkopf2001,TaxDuin2004,Chandola2009,Campos2016,Ruff2021}:
\[
\min \; \frac{1}{|V|} \sum_{z \in V} \Ind{ s_{\text{val}}(z) \ge 0 },
\qquad
s_{\text{val}}(z) \;=\; \sum_{i=1}^{n} \alpha_i\, K(z_i, z) \;-\; \rho .
\]
I tune over
\[
\gamma \in \left\{ \tfrac{t}{d} : t \in \{0.1, 0.3, 1, 3, 10\} \right\}
\;\cup\; \{\text{scale}, \text{auto}\}, \qquad
\nu \in \{0.01, 0.02, 0.05, 0.10, 0.20\},
\]
selecting by the validation criterion above, then report test ROCâ€“AUC and the maximum accuracy at the chosen hyperparameters and rank.

\subsection{Autoencoders}

Let \(\tilde{Z}_{\text{train}}\in\R^{|T|\times d}\). I train a fully connected autoencoder with hidden size \(H\in\{128,256,384\}\) and bottleneck \(B\in\{16,32,64\}\). The encoder maps \(\R^{d}\!\to\!\R^{H}\!\to\!\R^{B}\) and the decoder mirrors back to \(\R^{d}\) \parencite{Hinton2006,Goodfellow2016}:
\[
h_{\text{enc}} \;=\; E_{\theta}(\tilde{z}) \;\in\; \R^{B},\qquad
\hat{z} \;=\; D_{\theta}(h_{\text{enc}}) \;\in\; \R^{d}.
\]
Parameters \(\theta\) are learned by minimizing mean squared reconstruction error
\[
\theta^{\star}
\;=\;
\arg\min_{\theta}\;
\frac{1}{|T|}\sum_{\tilde{z}\in Z_{\text{train}}}
\frac{1}{d}\,\big\lVert \tilde{z} - f_{\theta}(\tilde{z}) \big\rVert_{2}^{2}.
\]
Anomaly scores are normalized reconstruction errors,
\[
s(\tilde{z}) \;=\; \frac{1}{d}\,\big\lVert \tilde{z} - f_{\theta}(\tilde{z}) \big\rVert_{2}^{2},
\]
with threshold set by a high validation quantile (e.g., P95) \parencite{Chandola2009,Campos2016,Ruff2021}.

\subsection{Isolation Forest}

Isolation Forest builds trees on subsamples of training features; anomalies are isolated in fewer steps \parencite{Liu2008,Liu2012}. I sweep
\[
T \in \{50,100,200\},\quad
p_{\text{sub}} \in \{0.5,0.75,1.0\},\quad
m_f \in \{0.5,0.75,1.0\},\quad
\text{contamination} \in \{0.05,0.10,0.20\},
\]
and use the standard score
\[
s(z) \;=\; 2^{-\bar{h}(z)/c(n_{\text{sub}})}, 
\qquad
\bar{h}(z) \;=\; \frac{1}{T}\sum_{t=1}^{T} h_t(z),
\]
with \(c(n_{\text{sub}})\) the usual normalization \parencite{Liu2008}. As with the autoencoder, I calibrate the threshold by a high validation quantile.

\printbibliography % APA-formatted references from refs.bib

\end{document}
