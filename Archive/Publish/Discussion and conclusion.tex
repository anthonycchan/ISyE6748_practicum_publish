\documentclass[11pt]{article}

% ---------- Formatting: Springer-like A4, two-column ----------
\usepackage[a4paper,left=18mm,right=18mm,top=22mm,bottom=24mm]{geometry}
\setlength{\columnsep}{5mm} % ~0.20in between columns
\usepackage{microtype}

% ---------- Packages ----------
\usepackage{amsmath,amssymb,mathtools,bm}
\usepackage{siunitx}

% ---------- APA citations ----------
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber,style=apa,sorting=nyt]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{refs.bib} % <-- ensure your BibTeX entries are here

% ---------- Macros ----------
% Sets & basic math
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}

% Bold vectors/matrices
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}

% Common symbols
\newcommand{\x}{\vect{x}}
\newcommand{\z}{\vect{z}}
\newcommand{\w}{\vect{w}}
\newcommand{\muvec}{\bm{\mu}}
\newcommand{\sigvec}{\bm{\sigma}}
\newcommand{\SigmaMat}{\bm{\Sigma}}
\newcommand{\Wmat}{\mat{W}}
\newcommand{\Lambdamat}{\bm{\Lambda}}

% Operators, norms, inner products
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\inner}[2]{\left\langle #1,\, #2 \right\rangle}

% Indicator and sign
\newcommand{\Ind}[1]{\mathbb{I}\!\left\{ #1 \right\}}
\DeclareMathOperator{\sign}{sign}

% Vectorization and PCA whitening
\DeclareMathOperator{\vecop}{vec}
\newcommand{\vecx}{\vecop}
\newcommand{\whiten}[1]{\Lambdamat^{-1/2}\Wmat^{\top} #1}

% Kernel & OC-SVM decision function
\newcommand{\rbf}[2]{\exp\!\big(-\gamma \norm{#1-#2}^{2}\big)}
\newcommand{\fsvm}{f}                % OC-SVM decision function
\newcommand{\score}[1]{-\,\fsvm(#1)} % anomaly-positive score s(z) = -f(z)

% Splits & bands
\newcommand{\train}{\textsc{Train}}
\newcommand{\val}{\textsc{Val}}
\newcommand{\final}{\textsc{Final}}
\newcommand{\bandmean}{\mu_b}
\newcommand{\bandsd}{\sigma_b}

% Metrics & thresholds
\newcommand{\AUC}{\mathrm{AUC}}
\newcommand{\FPzero}{\mathrm{FP@0}}
\newcommand{\topt}{\tau^\ast}

% Dimensions
\newcommand{\Hres}{64}
\newcommand{\Wres}{64}
\newcommand{\Bbands}{6}
\newcommand{\dfeat}{d} % post-processing feature dimension

% ---------- Formatting ----------
\usepackage{authblk} % for journal-style author/affiliation formatting

% ---------- Title ----------
\title{\textbf{From Tensors to Novelties: Low-Dimensional Representations for Anomaly Detection in Multispectral Imagery}}

\author{Anthony Chan}
\affil[1]{Georgia Institute of Technology, OMSA Practicum \\
\texttt{anthonycchan@gmail.com}}

\date{} % leave empty to omit date, or use \today

\begin{document}
\maketitle

\section{Discussion}

The experiments highlight both the potential and the limitations of tensor decompositions as feature extraction strategies for anomaly detection in multispectral imagery. Two evaluation setups were considered: (i) an in-order split using the first 1500 datapoints for training, and (ii) a randomized selection drawn from the full dataset. Together, these provide insight into the comparative strengths of CP and Tucker decompositions.  

\subsection{Improvements Under In-Order Splits}

When trained on the first 1500 datapoints in sequence, CP decomposition produced substantial improvements over baseline models. CP + OC-SVM in particular raised overall ROC-AUC from 0.56 (raw OC-SVM) to 0.71, with large category-level gains for bedrock, broken-rock, and veins (up to 0.40 AUC improvement). These results suggest that CP’s global rank structure is highly effective at capturing dominant spatial–spectral patterns when training data is locally coherent, enhancing separability for downstream classifiers such as OC-SVM and Isolation Forest. The optimal ranks identified for these models (\(R=120\) for CP + OC-SVM, \(R=35\) for CP + AE, and \(R=365\) for CP + IF) suggest that moderate to high global ranks were required to achieve maximal separation, emphasizing the capacity of CP to represent locally consistent spatial–spectral structure.  

\subsection{Robustness Under Randomized Sampling}

In contrast, when training samples were drawn at random across the full dataset, CP decomposition underperformed relative to baselines. CP + OC-SVM dropped to 0.49 ROC-AUC, below raw OC-SVM (0.56), with similar degradations for CP + AE and CP + IF. This indicates that CP’s single-rank factorization, while effective on homogeneous subsets, struggles to generalize under heterogeneous sampling.  

Tucker decomposition, however, demonstrated greater stability. Tucker + OC-SVM achieved 0.64 ROC-AUC, surpassing its baseline (0.56), while Tucker + AE also improved slightly (0.66 vs.\ 0.64). Gains were evident across multiple categories, though performance stagnated or declined for drill-hole, drt, and dump-pile. The optimal ranks for Tucker under randomized sampling—particularly \((64,16,5)\) for OC-SVM and \((64,64,5)\) for AE—indicate that higher spatial ranks improved flexibility while keeping compact spectral dimensions, contributing to its stability under heterogeneous conditions. These results highlight Tucker’s ability to flexibly allocate rank across modes, distributing spectral and spatial variability in a way that generalizes more effectively to diverse data. Isolation Forest, while strong as a baseline, showed limited benefit or degradation when paired with decompositions, suggesting it already captures variance structure effectively in raw feature space.  

\subsection{Category-Specific Recommendations}

The category-level analysis suggests that CP and Tucker decompositions offer complementary strengths. For locally coherent data, CP + OC-SVM yielded substantial improvements in anomalies such as bedrock, broken-rock, and veins, where global rank structure captures shared spectral–spatial patterns. For more heterogeneous anomaly types, Tucker decomposition proved more reliable, consistently outperforming baselines for categories including bedrock, broken-rock, float, scuff, and veins under randomized sampling. These findings indicate that decomposition choice can be tailored to the anomaly type of interest rather than applied as a one-size-fits-all strategy. These category-specific differences also align with the rank-search outcomes: CP required larger ranks to capture localized consistency, while Tucker achieved strong performance with compact yet balanced rank configurations.  

\subsection{Trade-Off Between Potential and Stability}

Overall, the two experiments reveal a trade-off between potential gain and robustness. CP decomposition provides striking improvements when training data is locally consistent and representative of test conditions, but its performance deteriorates under randomized sampling. Tucker decomposition delivers more modest improvements in structured splits yet maintains stable or superior performance under heterogeneous conditions. From a practical perspective, this robustness makes Tucker decomposition a stronger candidate for deployment in real-world scenarios where training data must capture diverse and unpredictable distributions.  

\subsection{Considerations}

While the current experiments demonstrate the utility of CP and Tucker decompositions for anomaly detection in multispectral imagery, several directions remain open. First, training was performed on relatively limited subsets (e.g., 1500 datapoints). Expanding the training pool could enable both CP and Tucker models to learn more representative latent factors, mitigating CP’s sensitivity to heterogeneity and strengthening generalization.  

Second, the number of anomaly test samples was limited. Incorporating additional labeled anomalies, either through annotation or synthetic augmentation, would allow more robust estimation of model sensitivity across categories and clarify whether CP’s advantages on structured anomalies and Tucker’s stability under heterogeneous data persist at scale.  

Finally, tensor decomposition methods should be applied to additional datasets. While this study focused on multispectral rover imagery, CP and Tucker factorizations are general techniques that extend to other domains such as medical imaging (MRI, CT) and industrial inspection. Benchmarking across diverse datasets will help determine whether the category-specific advantages observed here generalize and be broadly applicable.  


\section{Conclusion}
\label{sec:conclusion}

This work evaluated the use of tensor decomposition methods, specifically CP and Tucker factorizations, as feature extraction strategies for anomaly detection in multispectral imagery. By comparing models trained on raw pixel data with those trained on decomposed representations, two consistent findings emerged.    

First, CP decomposition demonstrated strong potential under locally coherent training conditions. In in-order splits, CP + OC-SVM in particular achieved substantial improvements, raising ROC-AUC by as much as 40\% in certain anomaly categories. These results highlight the power of global CP structure in capturing dominant spectral–spatial patterns when training and test distributions are aligned. The optimal ranks identified for the in-order setup (\(R=120\) for CP + OC-SVM and \(R=365\) for CP + IF) further underscore the role of high-rank global structure in modeling consistent data.  

Second, Tucker decomposition exhibited greater robustness under heterogeneous conditions. In randomized sampling, Tucker-based models consistently outperformed both baselines and CP-based pipelines, achieving the highest overall test set ROC-AUC and providing reliable gains across multiple categories. These outcomes suggest that Tucker’s ability to flexibly allocate rank across modes makes it more adaptable to diverse data distributions. Across experiments, the optimal rank search revealed that CP models favored higher ranks (e.g., 120–365), while Tucker models achieved competitive or superior results with lower, mode-specific ranks (e.g., \(32,32,16\) or \(64,16,5\)), underscoring Tucker’s efficiency in representing heterogeneous structure.  

Taken together, the results indicate a trade-off between potential and stability. CP offers the greatest performance gains in structured regimes, while Tucker provides more reliable performance in settings where variability and heterogeneity are high. From an applied perspective, this suggests that the choice of decomposition strategy should depend on the data characteristics and the anomaly categories of interest.  

Future work will involve scaling the training sets, expanding anomaly annotations, and applying these methods to additional domains such as medical imaging or industrial inspection. Such efforts will help determine whether the category-specific strengths observed here generalize across broader contexts.  

Overall, this study shows that tensor decompositions can substantially enhance anomaly detection pipelines, provided their use is tailored to the structure of the data and the deployment setting.


\end{document}
